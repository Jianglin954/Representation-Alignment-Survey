# Representation Potentials of Foundation Models for Multimodal Alignment: A Survey (EMNLP25)


[![Awesome Representation Alignment](https://awesome.re/badge.svg)](https://awesome.re)
![Paper Reading](https://img.shields.io/badge/Multimodal_Alignment-blue)

This is the repository for the survery paper: [Representation Potentials of Foundation Models for Multimodal Alignment: A Survey](). The collection will be continuously updated, so star (üåü) & stay tuned. Any suggestions and comments are welcome (jianglinlu@outlook.com). 



## Contents
- [Foundation Models](#FM)
  - [Vision Foundation Models](#VFMs)
  - [Large Language Models](#LLMs)
  - [Speech Foundation Models](#SFMs)
  - [Multimodal Foundation Models](#MFMs)
- [Alignment Metrics](#ALME)
  - [Centered Kernel Alignment](#CKA)
  - [Canonical Correlation Analysis](#CCA)
  - [Mutual Nearest Neighbors](#MNN)
- [Representation Convergence within Single-Modality](#RCSM)
  - [Vision](#RCSMV)
  - [Language](#RCSML)
  - [Speech](#RCSMS)
- [Representation Convergence within Across-Modalities](#RCAM)
- [Representation Convergence in Biological Systems](#RCBS)
- [Open Questions](#OpenQ)



<a name="FM" />

## Foundation Models [[Back to Top]](#)

1. **On the Opportunities and Risks of Foundation Models** *Rishi Bommasani et al, arXiv 2022.* [[PDF]](https://arxiv.org/pdf/2108.07258)




<a name="VFMs" />

### Vision Foundation Models

1. **Deep Residual Learning for Image Recognition** * Kaiming He et al, CVPR 2016.*  [[PDF]](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7780459) 

1. **An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale** *Alexey Dosovitskiy et al, ICLR 2021.*  [[PDF]](https://arxiv.org/pdf/2010.11929/1000)

1. **A ConvNet for the 2020s** *Zhuang Liu et al, CVPR 2022.*  [[PDF]](https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_A_ConvNet_for_the_2020s_CVPR_2022_paper.pdf) 

1. **ConvNeXt V2: Co-Designing and Scaling ConvNets With Masked Autoencoders** *Sanghyun Woo et al, CVPR 2023.*  [[PDF]](https://openaccess.thecvf.com/content/CVPR2023/papers/Woo_ConvNeXt_V2_Co-Designing_and_Scaling_ConvNets_With_Masked_Autoencoders_CVPR_2023_paper.pdf) 

1. **DINOv2: Learning Robust Visual Features without Supervision** *Maxime Oquab et al, arXiv 2024.*  [[PDF]](https://arxiv.org/pdf/2304.07193) 

1. **DINOv3.** *Oriane Sim√©oni et al, arXiv 2025.*  [[PDF]](https://arxiv.org/pdf/2508.10104) 

1. **Segment Anything** *Alexander Kirillov et al, ICCV 2023.*  [[PDF]](https://arxiv.org/pdf/2304.02643) 




<a name="LLMs" />

### Large Language Models

1. **Language Models are Few-Shot Learners** *Tom B. Brown et al, NeurIPS 2020.*  [[PDF]](https://arxiv.org/pdf/2005.14165)

1. **Scaling Laws for Neural Language Models** *Jared Kaplan et al, arXiv 2020.*  [[PDF]](https://arxiv.org/pdf/2001.08361)

1. **BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding** *Jacob Devlin et al, NAACL 2019.*  [[PDF]](https://aclanthology.org/N19-1423.pdf) 

1. **Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer** *Colin Raffel et al, JMLR 2020.*  [[PDF]](https://arxiv.org/pdf/1910.10683)

1. **Emergent Abilities of Large Language Models** *Jason Wei et al, TMLR 2022.*  [[PDF]](https://openreview.net/pdf?id=yzkSU5zdwD) 

1. **Qwen Technical Report** *Jinze Bai et al, arXiv 2023.*  [[PDF]](https://arxiv.org/pdf/2309.16609)

1. **The Llama 3 Herd of Models** *Aaron Grattafiori et al, arXiv 2024.*  [[PDF]](https://arxiv.org/pdf/2407.21783) 




<a name="SFMs" />

### Speech Foundation Models

1. **wav2vec: Unsupervised Pre-training for Speech Recognition** *Steffen Schneider et al, arXiv 2019.*  [[PDF]](https://arxiv.org/pdf/1904.05862)

1. **wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations** *Alexei Baevski et al, NeurIPS 2020.*  [[PDF]](https://proceedings.neurips.cc/paper/2020/file/92d1e1eb1cd6f9fba3227870bb6d7f07-Paper.pdf)

1. **HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units** *Wei-Ning Hsu et al, arXiv 2021.*  [[PDF]](https://arxiv.org/pdf/2106.07447)

1. **WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing** *Sanyuan Chen et al, arXiv 2022.*  [[PDF]](https://arxiv.org/pdf/2110.13900)

1. **Robust Speech Recognition via Large-Scale Weak Supervision** *Alec Radford et al, ICML 2023.*  [[PDF]](https://proceedings.mlr.press/v202/radford23a/radford23a.pdf)

1. **SeamlessM4T: Massively Multilingual & Multimodal Machine Translation** *Seamless Communication et al, arXiv 2023.*  [[PDF]](https://arxiv.org/pdf/2308.11596) 




<a name="MFMs" />

### Multimodal Foundation Models

1. **Learning Transferable Visual Models From Natural Language Supervision** *Alec Radford, et al, ICML 2021.*  [[PDF]](https://proceedings.mlr.press/v139/radford21a/radford21a.pdf) 

1. **Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision** *Chao Jia et al, ICML 2021.*  [[PDF]](https://arxiv.org/pdf/2102.05918) 

1. **Updating....** * et al, .*  [[PDF]]() 

1. **Updating....** * et al, .*  [[PDF]]() 

<a name="ALME" />

## Alignment Metrics [[Back to Top]](#)


1. **Updating....** * et al, .*  [[PDF]]() 

<a name="CKA" />

### Centered Kernel Alignment



<a name="CCA" />

### Canonical Correlation Analysis




<a name="MNN" />

### Mutual Nearest Neighbors



<a name="RCSM" />

## Representation Convergence within Single-Modality [[Back to Top]](#)

1. **Updating....** * et al, .*  [[PDF]]() 

<a name="RCSMV" />

### Vision

1. **Updating....** * et al, .*  [[PDF]]() 


<a name="RCSML" />

### Language

1. **Updating....** * et al, .*  [[PDF]]() 

<a name="RCSMS" />

### Speech

1. **Updating....** * et al, .*  [[PDF]]() 

<a name="RCAM" />

## Representation Convergence within Across-Modalities [[Back to Top]](#)

1. **Updating....** * et al, .*  [[PDF]]() 

<a name="RCBS" />

## Representation Convergence in Biological Systems [[Back to Top]](#)

1. **Updating....** * et al, .*  [[PDF]]() 

<a name="OpenQ" />

## Open Questions [[Back to Top]](#)


1. **Updating....** * et al, .*  [[PDF]]() 







## üìù Citation

If you find our survey useful, please consider citing:
``` bib file
@article{lu2025representations,
  title={Representation Potentials of Foundation Models for Multimodal Alignment: A Survey},
  author={Lu, Jianglin and Wang, Hailing and Xu, Yi and Wang, Yizhou and Yang, Kuo and Fu, Yun},
  booktitle={Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, {EMNLP} 2025},
  publisher={Association for Computational Linguistics},
  year={2024}
}
```

